export const metadata = {
  title: "Ollama 사용 방법",
  description: "Ollama 사용 방법에 대해 설명하는 페이지입니다.",
  date: new Date("2025-01-30"),
  category: "Raspberry Pi",
  tags: ["raspberry-pi", "linux", "ollama", "llm", "ai", "docker"],
  pin: false,
  imagePath: "/images/raspberry-pi/ollama/ollama.avif",
};

> **<u>Tags</u>**
>
> Raspberry Pi, Linux, Ollama, LLM, AI, Docker

> **<u>Environment</u>**
>
> Device: Raspberry Pi 4 Model B
>
> OS: Raspberry Pi OS (64 bit)
> RAM: 4GB

## 1. 개요

이번 글에서는 Raspberry Pi에 Ollama를 설치하고 사용하는 방법에 대해 설명하겠습니다.

## 2. Ollama란?

### 2.1. 개념

`Ollama`란 `대규모 언어 모델(Large Language Model, LLM)`을 로컬 환경에서 쉽게 실행하고 관리할 수 있도록 설계된 오픈소스 플랫폼 및 도구입니다.
Ollama는 Windows, macOS, Linux 같은 다양한 운영체제에서 설치 및 실행할 수 있으며,
Llama, Gemma, Mistral, DeepSeek 등 다양한 LLM을 지원합니다. 또한 REST API로 다른 애플리케이션과의 연동도 지원합니다.

### 2.2. Ollama의 특징

Ollama의 특징은 다음과 같습니다.

- `로컬 실행`  
  인터넷 연결 없이 로컬 환경에서 LLM을 사용할 수 있습니다.
- `다양한 모델 지원`  
  Ollama는 Llama, Gemma, Mistral, DeepSeek 등 다양한 LLM을 지원합니다.
- `간단한 설치 및 사용`  
  CLI를 통해 쉽게 설치 및 실행할 수 있습니다.
- `REST API 제공`  
  Ollama는 REST API를 제공하여 LLM을 다른 애플리케이션과 쉽게 연동할 수 있도록 지원합니다.
- `모델 커스터마이징 지원`  
  `GGUF(Georgi Gerganov Unified Format)` 형태의 모델을 임포트하거나 사용자 지정 모델을 학습시킬 수 있습니다.

### 2.3. 지원 모델

Ollama는 다양한 LLM을 지원합니다. 주요 LLM은 다음과 같습니다.

| Model              | Parameters | Size  | Download                         |
| ------------------ | ---------- | ----- | -------------------------------- |
| DeekSeek-R1        | 671B       | 404GB | `ollama run deepseek-r1:671b`    |
| DeekSeek-R1        | 70B        | 43GB  | `ollama run deepseek-r1:70b`     |
| DeekSeek-R1        | 32B        | 20GB  | `ollama run deepseek-r1:32b`     |
| DeekSeek-R1        | 14B        | 9.0GB | `ollama run deepseek-r1:14b`     |
| DeekSeek-R1        | 8B         | 4.8GB | `ollama run deepseek-r1:8b`      |
| DeekSeek-R1        | 7B         | 4.7GB | `ollama run deepseek-r1:7b`      |
| DeekSeek-R1        | 1.5B       | 1.1GB | `ollama run deepseek-r1:1.5b`    |
| Llama 3.3          | 70B        | 43GB  | `ollama run llama3.3`            |
| Llama 3.2          | 3B         | 2.0GB | `ollama run llama3.2`            |
| Llama 3.2          | 1B         | 1.3GB | `ollama run llama3.2:1b`         |
| Llama 3.2 Vision   | 11B        | 7.9GB | `ollama run llama3.2-vision`     |
| Llama 3.2 Vision   | 90B        | 55GB  | `ollama run llama3.2-vision:90b` |
| Llama 3.1          | 8B         | 4.7GB | `ollama run llama3.1`            |
| Llama 3.1          | 405B       | 231GB | `ollama run llama3.1:405b`       |
| Phi 4              | 14B        | 9.1GB | `ollama run phi4`                |
| Phi 3 Mini         | 3.8B       | 2.3GB | `ollama run phi3`                |
| Gemma 2            | 2B         | 1.6GB | `ollama run gemma2:2b`           |
| Gemma 2            | 9B         | 5.5GB | `ollama run gemma2`              |
| Gemma 2            | 27B        | 16GB  | `ollama run gemma2:27b`          |
| Mistral            | 7B         | 4.1GB | `ollama run mistral`             |
| Moondream 2        | 1.4B       | 829MB | `ollama run moondream`           |
| Neural Chat        | 7B         | 4.1GB | `ollama run neural-chat`         |
| Starling           | 7B         | 4.1GB | `ollama run starling-lm`         |
| Code Llama         | 7B         | 3.8GB | `ollama run codellama`           |
| Llama 2 Uncensored | 7B         | 3.8GB | `ollama run llama2-uncensored`   |
| LLaVA              | 7B         | 4.5GB | `ollama run llava`               |
| Solar              | 10.7B      | 6.1GB | `ollama run solar`               |

Ollama는 위의 표에 적힌 LLM 이외에도 다양한 LLM을 지원합니다.
사용할 수 있는 전체 LLM 목록은 [https://ollama.com/search](https://ollama.com/search)에서 확인하실 수 있습니다.

## 3. Step 1 - Ollama 설치하기

`Raspberry Pi OS (64 bit)`는 Linux 운영체제에 해당하므로 다음 2가지 방식으로 Ollama를 설치할 수 있습니다.
이번 글에서는 Docker로 설치하여 사용하는 방식을 선택하였습니다.

### 3.1. Linux에 설치하기

다음 명령어를 입력하여 Linux에 Ollama를 설치할 수 있습니다. 설치 중 NVIDIA/AMD GPU가 자동으로 감지되며, 해당 GPU가 없는 경우 CPU 모드로 동작합니다.

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### 3.2. Docker로 설치하기

Docker로 Ollama를 사용하기 위해선 먼저 Docker를 설치해야 합니다.
Docker 설치 방법은 [Docker의 개념과 사용 방법](/posts/2024-11-06-docker) 문서를 참고하시길 바랍니다.
Docker가 설치되어 있다면 다음 명령어를 입력하여 Docker Hub에서 Ollama 공식 이미지를 다운로드할 수 있습니다.

```bash
docker pull ollama/ollama
```

![Ollama 이미지 다운로드](/images/raspberry-pi/ollama/pic1.avif)

## 4. Step 2 - Ollama 실행하기

### 4.1. Linux에서 실행하기

다음 명령어를 입력하여 Ollama를 실행할 수 있습니다.

```bash
ollama serve
```

### 4.2. Docker로 실행하기

다음 명령어를 입력하여 Docker 이미지를 통해 Ollama를 실행할 수 있습니다.
해당 명령어는 `CPU`만 사용하는 방식입니다. GPU를 사용하는 방식은 다음 링크를 참고하시길 바랍니다.

[https://hub.docker.com/r/ollama/ollama](https://hub.docker.com/r/ollama/ollama)

```bash
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

## 5. Step 3 - Ollama 명령어 사용하기

이 문단에서는 주요 ollama 명령어에 대해 설명합니다.
Docker로 ollama를 사용하는 경우 각 명령어 앞에 `docker exec -it ollama`를 추가로 작성하거나 (Ex. `docker exec -it ollama` ollama list)
또는 `docker exec -it ollama bash` 명령어를 먼저 입력하여 interactive shell을 사용하면 됩니다.

### 5.1. Ollama 실행 - ollama serve

`ollama serve` 명령어를 입력하여 ollama를 실행할 수 있습니다. Docker로 Ollama를 설치한 경우 `docker run` 명령어 실행 시점에 자동으로 실행됩니다.

```bash
ollama serve
```

### 5.2. LLM 다운로드 - ollama pull

`ollama pull [모델명]` 명령어를 입력하여 LLM을 다운로드할 수 있습니다. 또한 해당 명령어는 로컬 모델을 업데이트할 때도 사용될 수 있습니다.

```bash
ollama pull llama3.2
```

![LLM 다운로드](/images/raspberry-pi/ollama/pic2.avif)

### 5.3. 다운로드한 LLM 목록 확인 - ollama list

`ollama list` 명령어를 입력하여 로컬에 다운로드한 LLM 목록을 확인할 수 있습니다.

```bash
ollama list
```

![다운로드한 LLM 목록 확인](/images/raspberry-pi/ollama/pic3.avif)

### 5.4. LLM 정보 확인 - ollama show

`ollama show [모델명]` 명령어를 입력하여 로컬에 다운로드한 특정 LLM 정보를 확인할 수 있습니다.

```bash
ollama show llama3.2
```

![LLM 정보 확인](/images/raspberry-pi/ollama/pic4.avif)

### 5.5. LLM 실행 - ollama run

`ollama run [모델명]` 명령어를 입력하여 LLM을 실행할 수 있습니다. 만약 로컬에 실행하고자 하는 LLM이 없는 경우 자동으로 다운로드합니다.

```bash
ollama run llama3.2
```

![LLM 실행](/images/raspberry-pi/ollama/pic5.avif)

### 5.6. 실행 중인 LLM 목록 확인 - ollama ps

`ollama ps` 명령어를 입력하여 현재 실행 중인 LLM 목록을 확인할 수 있습니다.

```bash
ollama ps
```

![실행 중인 LLM 목록 확인](/images/raspberry-pi/ollama/pic6.avif)

### 5.7. 실행 중인 LLM 중단 - ollama stop

`ollama stop [모델명]` 명령어를 입력하여 현재 실행 중인 LLM을 중단시킬 수 있습니다.

```bash
ollama stop llama3.2
```

![실행 중인 LLM 중단](/images/raspberry-pi/ollama/pic7.avif)

## 6. Step 4 - LLM 다운로드하기

Ollama를 실행한 후 LLM을 사용하기 위해선 먼저 LLM를 다운로드해야 합니다.
이번 글에서는 `Llama 3.2(3B, 2.0GM)`를 다운로드해서 실행하겠습니다.
다음 명령어를 입력하여 `Llama 3.2`을 다운로드합니다.

```bash
ollama pull llama3.2
```

![LLM 다운로드](/images/raspberry-pi/ollama/pic2.avif)

다운로드한 LLM 목록을 확인하기 위해 다음 명령어를 입력합니다.

```bash
ollama list
```

![LLM 목록 확인](/images/raspberry-pi/ollama/pic3.avif)

## 7. Step 5 - LLM 실행하기

LLM을 실행하기 위해 다음 명령어를 입력합니다.

```bash
ollama run llama3.2
```

LLM을 실행하면 다음과 같이 CLI에서 직접 모델과 채팅 형식으로 상호작용할 수 있습니다.

![LLM 실행](/images/raspberry-pi/ollama/pic5.avif)

## 8. Step 6 - REST API 사용하기

Ollama는 REST API를 제공하여 LLM을 다른 애플리케이션과 쉽게 연동할 수 있도록 지원합니다. 예를 들어 LLM과 대화를 하기 위해선 다음과 같이 호출할 수 있습니다.

![REST API 사용](/images/raspberry-pi/ollama/pic8.avif)

위에서 설명한 API 외의 다른 API도 지원합니다. 전체 API 목록은 다음 링크에서 확인하실 수 있습니다.

[https://github.com/ollama/ollama/blob/main/docs/api.md](https://github.com/ollama/ollama/blob/main/docs/api.md)

또한 위에서 사용한 방식이 아니라 library를 설치해서 API를 호출할 수도 있습니다.

- [https://github.com/ollama/ollama-js](https://github.com/ollama/ollama-js)
- [https://github.com/ollama/ollama-python](https://github.com/ollama/ollama-python)

## 9. 참고 자료

- [https://github.com/ollama/ollama](https://github.com/ollama/ollama)
- [https://ollama.com/](https://ollama.com/)
- [https://hub.docker.com/r/ollama/ollama](https://hub.docker.com/r/ollama/ollama)
